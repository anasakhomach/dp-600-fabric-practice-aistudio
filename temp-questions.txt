question_13: {{You are the administrator of a Fabric workspace that contains a lakehouse named Lakehouse1. Lakehouse1 contains the following tables:
Table1: A Delta table created by using a shortcut
Table2: An external table created by using Spark
Table3: A managed table -
You plan to connect to Lakehouse1 by using its SQL endpoint.
What will you be able to do after connecting to Lakehouse1?
A. Read Table3.
B. Update the data Table3.
C. Read Table2.
D. Update the data in Table1.
}}

After connecting to **Lakehouse1** using its **SQL analytics endpoint**, the correct action you will be able to perform is **A. Read Table3.**

### **Technical Reasoning and Source Cross-Reference**

*   **Read-Only Nature of the SQL Endpoint:** According to the sources, the SQL analytics endpoint is designed for **read access** using T-SQL. It allows users to "Query with T-SQL" and "View data in the Data preview". It does **not support Data Manipulation Language (DML)** operations such as `UPDATE`, `DELETE`, or `INSERT`. This immediately disqualifies options **B** and **D**.
*   **Support for Managed Tables:** **Table3** is a **managed table**. In a Fabric Lakehouse, managed tables are automatically stored in **Delta format**, which is a requirement for visibility in the SQL analytics endpoint. The endpoint automatically discovers these tables and surfaces them for T-SQL querying.
*   **External Spark Tables (Table 2):** **Table2** is an **external table created by using Spark**. Generally, the SQL analytics endpoint only surfaces tables that are in the **Delta Lake format** and registered in the Lakehouse. External tables created via Spark that do not meet the Delta specifications or reside in external locations without a proper shortcut may not be visible or readable via the SQL endpoint. 
*   **Delta Tables via Shortcuts (Table 1):** While **Table1** (a Delta table created via a shortcut) would be readable, the prompt asks for a valid action. Option **D** is incorrect because it suggests *updating* the data, which the read-only SQL endpoint cannot do.

### **Summary of SQL Endpoint Capabilities**

| Capability | Supported? | Source Evidence |
| :--- | :--- | :--- |
| **Read Data (Managed/Delta Tables)** | **Yes** | "Read access by using T-SQL" |
| **Query with T-SQL** | **Yes** | "Query with T-SQL... View data in Data preview" |
| **Update/Write Data** | **No** | Endpoint is read-only for T-SQL |
| **Python Access** | **No** (not via SQL endpoint) | Python access requires a Spark notebook |

**Analogy for SQL Endpoint vs. Spark:**
Think of the **SQL analytics endpoint** as a **Library's Reading Room**. You can walk in and read any of the books on the shelves (**Managed/Delta Tables**), but you are strictly forbidden from writing in them or changing the text (**Read-Only**). If you want to edit the books or add new chapters, you must go to the **Author's Studio (Spark Notebooks)**, which has the specialized tools for writing and revision.

question_14: {{
You have a Fabric tenant that contains a warehouse.
You use a dataow to load a new dataset from OneLake to the warehouse.
You need to add a PowerQuery step to identify the maximum values for the numeric columns.
Which function should you include in the step?
A. Table.MaxN
B. Table.Max
C. Table.Range
D. Table.Profile}}

To identify the maximum values for numeric columns within a Power Query step in a Fabric dataflow, you should use the function:

**D. Table.Profile**

### **Explanation and Logic**
*   **Table.Profile:** This Power Query (M) function is specifically designed to return a summary table containing statistical information for every column in the dataset. This summary includes the **Minimum, Maximum, Average, Standard Deviation, and Count** for each numeric column. Because your requirement is to identify maximum values for "numeric columns" (plural), `Table.Profile` is the most efficient way to generate a list of those maximums in a single step.
*   **Table.Max (Incorrect):** This function returns the largest **row** in a table based on a specific criteria, rather than a summary of maximum values for all columns.
*   **Table.MaxN (Incorrect):** This is used to return the top "N" number of rows from a table based on a sort order.
*   **Table.Range (Incorrect):** This function is used to return a subset of rows starting at a specific offset.

### **Alignment with Fabric Analytics Roles**
As a Fabric Analytics Engineer, you are expected to use **Dataflow Gen2** for data transformation and preparation tasks. One of the core skills measured for the DP-600 exam is the ability to **aggregate data** and **identify missing or null values**, which is often achieved through data profiling steps during the ingestion process. 

The sources note that the choice of data cleansing and transformation tools is at the discretion of the data engineer, but low-code tools like Dataflows are preferred for ingestion from sources like OneLake to the Warehouse.

***

**Note on Information Source:** While the provided sources confirm that **Dataflows** are the correct tool for these transformations and that **aggregating data** is a required skill, the specific technical definitions of Power Query "M" functions like `Table.Profile` are not explicitly detailed in these excerpts. This specific function knowledge is considered part of the prerequisite "Power Query" expertise required for the DP-600 certification. You may want to independently verify the M-language library for further function details.

**Analogy for Table.Profile:**
Think of **Table.Profile** like a **Medical Check-up Report** for your data. Instead of looking at every single patient (row) to see who is the tallest, the report gives you a summary page that lists the "Max Height," "Average Weight," and "Total Count" for every room (column) in the hospital at once.